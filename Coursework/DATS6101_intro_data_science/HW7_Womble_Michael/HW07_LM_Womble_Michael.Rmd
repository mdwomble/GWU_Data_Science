---
title: "Intro to DS - Linear Model part I"
author: ""
date: "today"
# date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
library(ezids)
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

# HW assignment

## Linear model - quantitative regressors 

### Question 1  
**Import the data, call it `bikeorig.`**  
The `Date` variable is probably imported as factor level variable. In any case, let us remove `Date`, `Casual.Users`, 
and `Registered.Users` in the dataset and save it as a new datafame, call it `bike`. How many variables are in `bike`? 
How many of them are imported as `int`? Feel free to rename longer variable names into shorter ones for convenience.  
```{r Q1}
library(dplyr)

#Load bike dataset.
bikedata<-data.frame(read.csv('bikedata.csv'))

#Remove Date, Casual users, and registered users.
col_remove<-c("Date", "Casual.Users", "Registered.Users")
bike<-bikedata%>%
  select(-one_of(col_remove))

#Rename several columns in the bike datarame.
bike <- bike %>%
  rename(weekday=Day.of.the.Week,
         workday=Working.Day,
         weather=Weather.Type,
         temp.F=Temperature.F,
         feels_like=Temperature.Feels.F,
         users=Total.Users)
str(bike)
```
**Answer:** There are 11 variables in the bike dataframe, nine of which are imported as integers.

### Question 2    
**Select only the subset with `Hour` equal 16 only. Call it `bike16`**  
These are the afternoon rush hour data. How many observations are there?
```{r Q2}
bike16<-subset(bike, Hour==16)
str(bike16)
```
**Answer:** There are 730 observations in this new bike16 dataframe.

### Question 3  
**Before building any models, we should make sure the variables are set up properly.**  
(This problem is solved for you. Codes are given below.)  
Which ones should be recorded as categorical? Convert them now before we proceed to the model building.  

Note: After converting, the correlation function `cor()` will not work with categorical/factor variables. 
I would keep the original `bike16` dataframe as numeric, and use that to 
find the correlation matrix. 
Although technically correlation between categorical and numeric variables are not 
well defined in general, we can still get some useful information if the 
categorical variable is at least at ordinal level. See future discussion 
on using "Pearson" vs "Spearman" methods for correlation tests. 

While the `cor()` function does not accept categorical variables (and therefore 
we cannot use it for `corrplot()`), the `lattice::pairs()` function does not complain 
about categorical columns. We can still use it to get a visual distribution of 
data values from it.
 

```{r}
bike_final = bike16
bike_final$Season = factor(bike16$Season)
#bike_final$Hour = factor(bike16$Hour)
bike_final$Holiday = factor(bike16$Holiday)
bike_final$weekday = factor(bike16$weekday)
bike_final$workday = factor(bike16$workday)
bike_final$weather = factor(bike16$weather)
str(bike_final)
```
We decided to convert these variables into categorical (factor):  
`Season`, `Holiday`, `Day`, `Workday`, and `Weather`.  Notice that 
the dataframe `bike16` still has all variables numerical, while the df `bike_final` 
include categorical columns that we just converted. 

**Answer:** The season, holiday, weekday, workday, and weather variables should be recorded as categorical variables.

### Question 4  
**Make a `pairs()` plot with all the variables (quantitative and qualitative).**
```{r Q4}
pairs(bike16)
```
**Figure 1.** Pair plot of the variables in the bike16 data set.

### Question 5  
**Make a `corrplot()` with only the numerical variables.**  
You can either subset the df with only numerical variables first, then create 
the create the cor-matrix to plot. Or you can create the cor-matrix from 
`bike16`, then select select out the portion of the matrix that you want. 
Use options that shows well the relationships between different variables. 
```{r Q5}
library(corrplot)
corrplot(cor(bike16[3:11]), method='number', type = 'upper')
```
**Figure 2.** Correlation matrix for variables in the bike16 dataframe, excluding season and hour variables.

### Question 6   
**By using numerical variables only, build a linear model with 1 independent variable to predict the `Total Users`.**  
Choose the variable with the strongest correlation coefficient. Make some short 
comments on the coefficient values, their p-values, and the multiple R-squared value.  
```{r Q6}
fit1<-lm(users~feels_like, data=bike16)
summary(fit1)
xkabledply(fit1, title = paste("Model :", format(formula(fit1)) ))
```
**Answer:** I used the feels_like temperature variable as the independent variable. The variable coefficient is estimated to have a value of 4 while the intercept is estimated to be 44.5. This fit produces a multiple R-squared value of 0.309 and a p value < 2e-16.

### Question 7   
**Next, add a second variable to the model.**  
Choose the variable with the next strongest correlation, but avoid using obviously 
collinear variables. When you have the model, 
check the VIF values. If the VIF is higher than 5, discard this model, and try the 
variable with the next strongest correlation until you find one that works 
(ideally with vif’s <5, or if you have to, allow vif up to 10). Again, 
comment on the coefficient values, their p-values, 
and the multiple R-squared value.  
```{r Q7}
fit2<-lm(users~feels_like+weather, data=bike16)
summary(fit2)
xkablevif(fit2)
```
**Answer:** I used the feels_like temperature and the weather variables as the independent variables. The feels_like and weather variable coefficients are estimated to have values of 3.71 and -47.37, respectively, while the intercept is estimated to be 131.6. This fit produces a multiple R-squared value of 0.352 which is an improvement on fit1 and a p value < 2e-16, comparable with fit1.

### Question 8  
**We will try one more time as in the previous question, to add a third variable in our model.**  
```{r Q8}
fit3<-lm(users~feels_like+weather+Humidity, data=bike16)
summary(fit3)
xkablevif(fit3)
```
**Answer:** I used the feels_like temperature, weather, and humidity variables as the independent variables. The feels_like, weather, and humidity variable coefficients are estimated to have values of 3.73, -27.84, and -1.084, respectively, while the intercept is estimated to be 156.54. This fit produces a multiple R-squared value of 0.363 which is an improvement on fit2 and a p value < 2e-16, comparable with fit1.

### Question 9  
**For the 3-variable model you found, find the confidence intervals of the coefficients.**
```{r Q9, results='markup'}
xkabledply(confint(fit3), title = '95% Confidence Intervals for the Variable Coefficients in Fit3')
```

### Question 10    
**Use ANOVA to compare the three different models you found.**  
You have found three different models. Use ANOVA test to compare their residuals. What conclusion can you draw?
```{r Q10}
anova_res<-anova(fit1,fit2,fit3)
anova_res
str(anova_res)
```

```{r Q10b, results='markup'}
xkabledply(anova_res, title = "ANOVA results comparing fits 1, 2, & 3")
```
**Answer:** From the reduction in the residual sum of squares (RSS) values with increasing number of independent variables the fit gets better the more independent variables are used. However, we must always be careful of over fitting our data and should not just continuously add more independent variables.



